# 快速入门

我们依然以抓取大众点评为例，讲解使用etlpy的全过程。在这个例子里，可以学习到：

- etlpy的核心思路
- 从网页中自动嗅探信息
- 设置cookie来伪装成浏览器
- 如何保存结果到文件
- 设置代理防爬

先安装etlpy: 
`pip install etlpy`
完整的示例代码在：
> 

注意事项：

1. 大众点评对于列表页的访问不会禁止，但对详情页的过度访问会导致被封禁。

## 1. 最简单的例子：

先访问一个搜索页面的html:
```
from etlpy.etlpy import *
url='http://www.dianping.com/'
url_s= '/search/category/3/75/g2878'
t = task().create().url.set(url + url_s).get()
for data in t:
    print(t)
```

etlpy会默认创建一个工程，可在工程中创建task，task函数的参数可以指定该task的名字。之后的每个操作都是算子(op)，op分为以下几类： 
- 生成器
- 转换器
- 过滤器
- 执行器

我们先通过create创建一张空表，行数为1，列数为0，`.url`表示针对url列（由于url列之前不存在，所以新增）,设置它的值为`url+url_s`，之后用get函数获取这个url的Html。etlpy在请求方面高度模拟了requests库，因此还能post。

> get不传入参数，是如何知道要获取什么url？
当你设置了url这个列之后，除非通过某些算子改变到新的列，否则之后所有的op都是针对该列进行的。get算子会把html内容覆盖到url列里。
> 为什么要用create跟在task后面？
任何task都需要通过生成器作为起点生成数据，create可以接受字典数组，pandas对象等参数，创建一个数据表。

结果是个字典的迭代器，你可以只打出网页的html:
`print(t['url'])`

如果希望保留url列，把获得的html放在另外一列，则可以：

`t = task().create().url.set(url + url_s).cp('_:html').get()`

它指代将url列的内容拷贝到html列，之后在html列上get数据。cp类似linux的cp操作，拷贝后，对列的指针就会移动到html列了。


## 2. 获取网页的列表信息

下面体验etlpy一个非常强大的功能：自动嗅探，对该列表页，我们希望能获得它的列表内容，在后面跟detect算子：

`t = task().create().url.set(url + url_s).get().detect()`

它会打印出下面的信息：

```
.xpath('/html/body/div/div[4]/div[6]/div').list().html().tree()\
.cp('url:diggnum').xpath('//div[1]/div[1]/span')[0].text()\
.cp('url:titlelnk').xpath('//div[2]/h3/a')[0].text()\
.cp('url:post_item_summary').xpath('//div[2]/p')[0].text()\
.cp('url:lightblue').xpath('//div[2]/div/a')[0].text()\
.cp('url:gray').xpath('//div[2]/div/span[1]/a')[0].text()\
.cp('url:col5').xpath('//div[2]/div/span[2]/a')[0].text()\
#diggnum : #1
#titlelnk : #NodeJs之数据库异常处理
#post_item_summary : #
#lightblue : #leslie·Zhao
#gray : #评论(0)
#col5 : #阅读(70)
```
这段代码当你有经验之后，可以自己手工编写。我们先不急解说它的原理，先把它按部就班地拷贝到之前流的后面，注意要删掉detect
```
t = task().create().url.set(url + url_s).get()\
.xpath('/html/body/div/div[4]/div[6]/div').list().html().tree()\
.cp('url:diggnum').xpath('//div[1]/div[1]/span')[0].text()\
.cp('url:titlelnk').xpath('//div[2]/h3/a')[0].text()\
.cp('url:post_item_summary').xath('//div[2]/p')[0].text()\
.cp('url:lightblue').xpath('//div[2]/div/a')[0].text()\
.cp('url:gray').xpath('//div[2]/div/span[1]/a')[0].text()\
.cp('url:col5').xpath('//div[2]/div/span[2]/a')[0].text()
```
然后再执行，是不是整个列表页都能获取了？结果如下。

先解释下xpath算子，它能解析html，并将对应xpath的列表提取出来，但由于它会返回多个内容（因为满足xpath语法的节点可能不止一个），使用list算子可以将内部列表扩展到外部的表格（参考备注1）。

之后每个数据都是etree节点，使用html算子将其转换为html，再转换为树节点。

我们重点解释下面的脚本，text算子能够将某etree节点的文本提取出来。

cp('url:diggnum').xpath('//div[1]/div[1]/span')[0].text() 

由于有多个属性列，因此可通过多个上面的表达式级联来获得所有的属性。

如果你希望获取某个节点的html，则可以将text算子改成html。


新增的列属性名是通过自动推断来获得的，可修改cp中的`:`来设置新的列名。


## 3. 设置cookie

由于get/requests请求需要众多其他的参数，例如cookie,hosts等，甚至还要不停地切换代理，多个算子可能会共享相同参数，也可能在参数上会有细微的不同。因此如何向etlpy的算子中传递参数就成了有趣的问题。

我们可将参数表达成一棵树，通过构造表达式树，etlpy在执行时才会对树的叶子节点求值，并传递给算子。废话不多说，通过下面的例子直接说明：

```
params='''Accept-Encoding:gzip, deflate
Host:www.dianping.com
User-Agent:Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_6) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/60.0.3112.113 Safari/537.36'''
headers = para_to_dict(cookie, '\n', ':')
r = request_param
r = r.merge('headers', headers)
```
request_param为系统默认的参数，通过将字符串转换为字典，并将该headers传入，就实现了树的构建。
之后将r直接传入到get/post算子的参数中即可。

若要设置代理呢？ 
通过查看requests库的文档，requests需要以如下方式设置代理:
```
r= requests.get(url,proxies={'http://ip:port'})
```
因此，下面实现了一个动态的代理模块：
--补充代码

随机求值器会在每次执行时动态获取代理，从而实现代理的均匀调用。

## 4. 如何设置翻页？

## 5. 获取所有类目的所有数据

## 6. 写入数据

我们可以将生成的数据保存到文件，或导出到pandas的Dataframe.
.to_df()，也可以写入json

但是，若数据量特别大，而且无法进行分布式时

## 7. 并行执行

希望获取大众点评全网的数据时，用上述脚本执行会非常的慢。因此我们需要考虑并行化。

etlpy对并行化的使用简单地令人发指。在query或execute参数上，可设置mode=PROCESS_MODE（多进程模式），它就会分析算子序列，分析生成结果，自动在其中插入并行算子。

如果你对它生成的结果不满意，也可以手工地添加pl算子，也可以强行指定算子的执行模式（多少个worker,多少条数据为一个分组，多线程/多进程）。 之后etlpy执行引擎会将整个流切分成一段段，构造为带层级关系的map-reduce工作流。

并行模式涉及到不少细节，也可能导致意想不到的问题。这一部分我们会在并行化中详细介绍。





