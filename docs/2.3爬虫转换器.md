# 爬虫转换器

etlpy针对爬虫进行了特定的优化，提供了一系列算子

## 访问网页数据：get,post

`url.set('www.cnblogs.com').cp('_:content').get()`

将先将url保存在url列，访问web,再将获取的content保存回p列。

如果想发送post请求，post数据可以以字典形式放在某列，作为参数传给post算子，如:

`p.set({'a':1,'b':2}).url.set('www.baidu.com').post('[p]')`

> 此处使用了括号表达式

代理的使用方法，参考本文最下方有关代理的相关内容


## tree,xpath,pyq

通过get,post方法获取的是原始的html文本，通过tree算子,可以将其加载为基于lxml的node节点。

之后，就可以通过xpath和pyquery获取html节点里的数据了。

例如: `url.set('www.cnblogs.com').tree().xpath('//div[2]')[0]`

xpath, pyq算子能分别接受xpath表达式和jquery表达式，得到的结果是**满足条件的算子的节点数组**。

上面可以省略tree，因为xpath,pyq会检查输入的目标列的值，如果为字符串会自动解析为tree。 提前使用tree是为了满足从html中多次解析数据的需要，例如：

`url.set('www.cnblogs.com').get().tree().cp('_:d1').xpath('//div[2]').cp('url:d2').xpath('//p[2]')`

这样html只会被解析一次(获取的节点一直保存在url列中)，避免被解析多次，因为解析比较消耗性能，因此尽量显式使用tree算子。


> xpath和jquery语法请参考相关文档

> 如果tree的目标列已经是节点类型，则不做转换


## html和text

对于node节点，你可以调用html算子和text算子，分别获取节点的html或文本。因此，html()等价于dump('html'),tree()等价于load('html'),它们互为逆操作。

> 当输入列为其他类型时，text算子会对其执行str()操作。

一个常见的需求是，需要从网页中提取正文(新闻等)，tree算子可以接受smart参数，自动返回包含最优全文的节点，之后可以将其转换为html或text:

`...html.tree(smart=True).html()`



## 自动嗅探和搜索

由于编写xpath依然费时费力，etlpy提供了智能的数据探测器，在tree算子之后，你可以使用detect算子，自动输出最佳的列表数据，并打印出对应的etlpy代码。例如：

`url.set('http://www.cnblogs.com').get().tree().detect()`

还可以在tree算子之后，执行search,参数为关键字，该算子会输出对应的xpath.

## 下载文件

dl算子用于下载文件。目标列保存超链接，算子参数为要保存的路径，例如:

`path.set('save_path').url.set('url').dl('[path]')`

TODO: 如果要加入其它cookie，则。。。。



## 使用代理

使用代理可以防止爬虫被网页拦截，此乃杀人越货必备佳品。

在流中，可能会使用多个get/post算子去访问同一个网站，因此对每个get,post单独设置代理就变得非常繁琐了。因此可以在工程的环境(env)中设置代理管理器，该管理器会拦截get/post/dl算子的执行，为其注入代理的代码。

在默认工程中，可调用如下函数创建名为my_proxy的代理:

```
def set_proxy(name='my_proxy',proxy=None,header=None,delay=0.1,agent=True,timeout=20,allow_local=True):
    pass
```
其中proxy参数接受一个字符串数组作为列表：

proxy= ['http://10.244.0.8:2398']

之后，所有proxy设置为my_proxy的web请求算子，都会自动被该代理拦截：

`task()...get(proxy='my_proxy')....post(proxy='my_proxy')`

当proxy数组不为空时，系统会会自动从该列表中随机选取代理去请求数据。

TODO: 研究如何传入多个命名参数

